{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9e42b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "#Langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ['LANGCHAIN_TRACKING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv('LANGCHAIN_PROJECT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bb9b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x20d64a62510>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Data Ingestion - Scrap data from the website\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.langchain.com/langsmith/data-storage-and-privacy\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd50c26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content=\"Data storage and privacy - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangSmithSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationData managementData storage and privacyGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APIAPI reference for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementData storage and privacyData purging for complianceScalability & resilienceAuthentication methodsFAQsRegions FAQPricing FAQOn this pageCLILangGraph ServerLangSmith TracingIn-memory development serverStandalone ServerStudioQuick referenceAdditional resourcesData managementData storage and privacyCopy pageCopy pageThis document describes how data is processed in the LangGraph CLI and the LangGraph Server for both the in-memory server (langgraph dev) and the local Docker server (langgraph up). It also describes what data is tracked when interacting with the hosted Studio frontend.\\n\\u200bCLI\\nLangGraph CLI is the command-line interface for building and running LangGraph applications; see the CLI guide to learn more.\\nBy default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process’s OS, OS version, Python version, the CLI version, the command name (dev, up, run, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic here.\\nYou can disable all CLI telemetry by setting LANGGRAPH_CLI_NO_ANALYTICS=1.\\n\\n\\u200bLangGraph Server\\nThe LangGraph Server provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for langgraph dev) or a PostgreSQL database (for langgraph up and in all deployments).\\n\\u200bLangSmith Tracing\\nWhen running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting LANGSMITH_TRACING=false in your server’s runtime environment.\\n\\n\\u200bIn-memory development server\\nlanggraph dev runs an in-memory development server as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a .langgraph_api directory in the current working directory. Apart from the telemetry data described in the CLI section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.\\n\\n\\u200bStandalone Server\\nlanggraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\\nAdditional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).\\nIf you’ve disabled tracing, no user data is persisted externally unless your graph code explicitly contacts an external service.\\n\\u200bStudio\\nStudio is a graphical interface for interacting with your LangGraph server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at smith.langchain.com, it is run in your browser and connects directly to your local LangGraph server so that no data needs to be sent to LangSmith.\\nIf you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:\\n\\nPage visits and navigation patterns\\nUser actions (button clicks)\\nBrowser type and version\\nScreen resolution and viewport size\\n\\nImportantly, no application data or code (or other sensitive configuration details) are collected. All of that is stored in the persistence layer of your LangGraph server. When using Studio anonymously, no account creation is required and usage analytics are not collected.\\n\\u200bQuick reference\\nIn summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.\\nVariablePurposeDefaultLANGGRAPH_CLI_NO_ANALYTICS=1Disable CLI analyticsAnalytics enabledLANGSMITH_API_KEYEnable LangSmith tracingTracing disabledLANGSMITH_TRACING=falseDisable LangSmith tracingDepends on environment\\n\\nEdit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoRelease versionsPreviousData purging for complianceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load into a document\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27800ccd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content=\"Data storage and privacy - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangSmithSearch...⌘KGitHubTry LangSmithTry LangSmithSearch...NavigationData managementData storage and privacyGet startedObservabilityEvaluationPrompt engineeringDeploymentAgent BuilderHostingOverviewPlansCreate an account and API keyAccount administrationOverviewSet up a workspaceManage organizations using the APIManage billingSet up resource tagsUser managementReferenceLangSmith Python SDKLangSmith JS/TS SDKLangGraph Python SDKLangGraph JS/TS SDKLangSmith APIAPI reference for LangSmith DeploymentAdditional resourcesReleases & changelogsData managementData storage and privacyData purging for complianceScalability & resilienceAuthentication methodsFAQsRegions FAQPricing FAQOn this pageCLILangGraph ServerLangSmith TracingIn-memory development serverStandalone ServerStudioQuick referenceAdditional resourcesData\"),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='& resilienceAuthentication methodsFAQsRegions FAQPricing FAQOn this pageCLILangGraph ServerLangSmith TracingIn-memory development serverStandalone ServerStudioQuick referenceAdditional resourcesData managementData storage and privacyCopy pageCopy pageThis document describes how data is processed in the LangGraph CLI and the LangGraph Server for both the in-memory server (langgraph dev) and the local Docker server (langgraph up). It also describes what data is tracked when interacting with the hosted Studio frontend.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bCLI\\nLangGraph CLI is the command-line interface for building and running LangGraph applications; see the CLI guide to learn more.\\nBy default, calls to most CLI commands log a single analytics event upon invocation. This helps us better prioritize improvements to the CLI experience. Each telemetry event contains the calling process’s OS, OS version, Python version, the CLI version, the command name (dev, up, run, etc.), and booleans representing whether a flag was passed to the command. You can see the full analytics logic here.\\nYou can disable all CLI telemetry by setting LANGGRAPH_CLI_NO_ANALYTICS=1.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph Server\\nThe LangGraph Server provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for langgraph dev) or a PostgreSQL database (for langgraph up and in all deployments).\\n\\u200bLangSmith Tracing\\nWhen running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting LANGSMITH_TRACING=false in your server’s runtime environment.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIn-memory development server\\nlanggraph dev runs an in-memory development server as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a .langgraph_api directory in the current working directory. Apart from the telemetry data described in the CLI section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bStandalone Server\\nlanggraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\\nAdditional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='If you’ve disabled tracing, no user data is persisted externally unless your graph code explicitly contacts an external service.\\n\\u200bStudio\\nStudio is a graphical interface for interacting with your LangGraph server. It does not persist any private data (the data you send to your server is not sent to LangSmith). Though the Studio interface is served at smith.langchain.com, it is run in your browser and connects directly to your local LangGraph server so that no data needs to be sent to LangSmith.\\nIf you are logged in, LangSmith does collect some usage analytics to help improve the debugging user experience. This includes:'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='Page visits and navigation patterns\\nUser actions (button clicks)\\nBrowser type and version\\nScreen resolution and viewport size\\n\\nImportantly, no application data or code (or other sensitive configuration details) are collected. All of that is stored in the persistence layer of your LangGraph server. When using Studio anonymously, no account creation is required and usage analytics are not collected.\\n\\u200bQuick reference\\nIn summary, you can opt-out of server-side telemetry by turning off CLI analytics and disabling tracing.\\nVariablePurposeDefaultLANGGRAPH_CLI_NO_ANALYTICS=1Disable CLI analyticsAnalytics enabledLANGSMITH_API_KEYEnable LangSmith tracingTracing disabledLANGSMITH_TRACING=falseDisable LangSmith tracingDepends on environment'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='Edit the source of this page on GitHub.\\nConnect these docs programmatically to Claude, VSCode, and more via MCP for    real-time answers.Was this page helpful?YesNoRelease versionsPreviousData purging for complianceNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split document into chunks, there is context size so we need to divide into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2eb020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the documents into vectors\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30ae7a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store into a VectorDB\n",
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstoredb = FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e409c2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "​Standalone Server\n",
      "langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\n",
      "Additional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).\n"
     ]
    }
   ],
   "source": [
    "# Query the vectorDB\n",
    "query = \"langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36694be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "​Standalone Server\n",
      "langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\n",
      "Additional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).\n"
     ]
    }
   ],
   "source": [
    "# Query the vectorDB\n",
    "query = \"langgraph three containers\"\n",
    "result = vectorstoredb.similarity_search(query)\n",
    "print(result[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8abf5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model = \"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b27ec3e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000020D7B66FE90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020D79EE1590>, root_client=<openai.OpenAI object at 0x0000020D7B7F2E10>, root_async_client=<openai.AsyncOpenAI object at 0x0000020D7B66EAD0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrieval chain, Document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "## Chatprompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "\"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5ebb006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can you encrypt checkpoints before saving them to the database in langgraph?\\n\\nYou can encrypt checkpoints before saving them to the database by setting a valid `LANGGRAPH_AES_KEY` environment variable.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke(\n",
    "{\"input\": \"langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers\",\n",
    "\"context\": [Document(page_content = \"langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\")]\n",
    "}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1fa57086",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstoredb.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1471eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000020D75454790>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000020D7B66FE90>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000020D79EE1590>, root_client=<openai.OpenAI object at 0x0000020D7B7F2E10>, root_async_client=<openai.AsyncOpenAI object at 0x0000020D7B66EAD0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21858241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are the primary functions of the PostgreSQL and Redis containers in the \"Standalone Server\"?\\n\\nThe PostgreSQL container in the \"Standalone Server\" is used to store all persistent data, including checkpoints, assistants, etc. The Redis container is used as a pub/sub connection for real-time streaming of events.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the response from the LLM\n",
    "response = retrieval_chain.invoke({\"input\":\"langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d6e8c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'langgraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers',\n",
       " 'context': [Document(id='3783fb39-4a9e-4297-a399-1b744c8773c1', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bStandalone Server\\nlanggraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\\nAdditional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).'),\n",
       "  Document(id='f12247a1-806e-485f-8fe1-19b8d9680fab', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph Server\\nThe LangGraph Server provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for langgraph dev) or a PostgreSQL database (for langgraph up and in all deployments).\\n\\u200bLangSmith Tracing\\nWhen running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting LANGSMITH_TRACING=false in your server’s runtime environment.'),\n",
       "  Document(id='c73bfc46-c431-42e3-a484-999ca9d1a60f', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='& resilienceAuthentication methodsFAQsRegions FAQPricing FAQOn this pageCLILangGraph ServerLangSmith TracingIn-memory development serverStandalone ServerStudioQuick referenceAdditional resourcesData managementData storage and privacyCopy pageCopy pageThis document describes how data is processed in the LangGraph CLI and the LangGraph Server for both the in-memory server (langgraph dev) and the local Docker server (langgraph up). It also describes what data is tracked when interacting with the hosted Studio frontend.'),\n",
       "  Document(id='10175290-5fe1-44fa-af5c-d8342e2c0450', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIn-memory development server\\nlanggraph dev runs an in-memory development server as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a .langgraph_api directory in the current working directory. Apart from the telemetry data described in the CLI section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.')],\n",
       " 'answer': 'What are the primary functions of the PostgreSQL and Redis containers in the \"Standalone Server\"?\\n\\nThe PostgreSQL container in the \"Standalone Server\" is used to store all persistent data, including checkpoints, assistants, etc. The Redis container is used as a pub/sub connection for real-time streaming of events.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c07cae28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='3783fb39-4a9e-4297-a399-1b744c8773c1', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bStandalone Server\\nlanggraph up builds your local package into a Docker image and runs the server as the data plane consisting of three containers: the API server, a PostgreSQL container, and a Redis container. All persistent data (checkpoints, assistants, etc.) are stored in the PostgreSQL database. Redis is used as a pubsub connection for real-time streaming of events. You can encrypt all checkpoints before saving to the database by setting a valid LANGGRAPH_AES_KEY environment variable. You can also specify TTLs for checkpoints and cross-thread memories in langgraph.json to control how long data is stored. All persisted threads, memories, and other data can be deleted via the relevant API endpoints.\\nAdditional API calls are made to confirm that the server has a valid license and to track the number of executed runs and tasks. Periodically, the API server validates the provided license key (or API key).'),\n",
       " Document(id='f12247a1-806e-485f-8fe1-19b8d9680fab', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bLangGraph Server\\nThe LangGraph Server provides a durable execution runtime that relies on persisting checkpoints of your application state, long-term memories, thread metadata, assistants, and similar resources to the local file system or a database. Unless you have deliberately customized the storage location, this information is either written to local disk (for langgraph dev) or a PostgreSQL database (for langgraph up and in all deployments).\\n\\u200bLangSmith Tracing\\nWhen running the LangGraph server (either in-memory or in Docker), LangSmith tracing may be enabled to facilitate faster debugging and offer observability of graph state and LLM prompts in production. You can always disable tracing by setting LANGSMITH_TRACING=false in your server’s runtime environment.'),\n",
       " Document(id='c73bfc46-c431-42e3-a484-999ca9d1a60f', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='& resilienceAuthentication methodsFAQsRegions FAQPricing FAQOn this pageCLILangGraph ServerLangSmith TracingIn-memory development serverStandalone ServerStudioQuick referenceAdditional resourcesData managementData storage and privacyCopy pageCopy pageThis document describes how data is processed in the LangGraph CLI and the LangGraph Server for both the in-memory server (langgraph dev) and the local Docker server (langgraph up). It also describes what data is tracked when interacting with the hosted Studio frontend.'),\n",
       " Document(id='10175290-5fe1-44fa-af5c-d8342e2c0450', metadata={'source': 'https://docs.langchain.com/langsmith/data-storage-and-privacy', 'title': 'Data storage and privacy - Docs by LangChain', 'language': 'en'}, page_content='\\u200bIn-memory development server\\nlanggraph dev runs an in-memory development server as a single Python process, designed for quick development and testing. It saves all checkpointing and memory data to disk within a .langgraph_api directory in the current working directory. Apart from the telemetry data described in the CLI section, no data leaves the machine unless you have enabled tracing or your graph code explicitly contacts an external service.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8780f182",
   "metadata": {},
   "outputs": [],
   "source": [
    "ChatPromptTemplate\n",
    "document_chain\n",
    "create_stuff_documents_chain\n",
    "retrieval_chain\n",
    "create_retrieval_chain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlbootcamp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
